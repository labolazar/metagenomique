---
title: "Protocol for metagenomic analyses"
author: "Karine Villeneuve"
date: "`r Sys.Date()`"
output: rmdformats::robobook
---

# Read me 

This protocol was created with the help of the baker lab team. Most of the programs are already installed on our server thanks to Mélanie Dominique and Karine Villeneuve. 

This protocol is intended to guide you through the different steps required to assemble and annotate reads from shotgun sequencing. 

Some program may take some time to process, we therefor recommend running the command using a shell script with nohup. For exemple on how to use, see section `Nohup`. 

# Pre-Assembly 

## Unzip 
Open a terminal window and move to the folder with your raw `fastq.gz`. 
```{bash, eval=FALSE}
gunzip *.gz
```

## Interleaving 
I used the script <font color='green'>interleave.py</font> from this [GitHub gist](https://gist.github.com/ngcrawford/2232505). The script is currently available in the folder `Script Métagénomique` of our Teams (see `General`). Transfer the script to the folder with the fastq files on the server, make the script executable and run the script using python. 

Make the script executable and chose option **a** or **b** depending on the amount of fastq you have. 
```{bash, eval=FALSE}
chmod +x interleave.py
```

a. If you have only a few fastq use this simple command line. 
```{bash, eval=FALSE}
python3 interleave.py file_R1.fastq file_R1_fastq > interleaved.fastq
```

b. If you have multiple fastq use this loop with nohup and a bash script.
``````{bash, eval=FALSE}
for R1 in *_R1_001.fastq ; do python3 interleave_fastq.py $R1 "${R1/R1/R2}"  > $R1.interleave.fastq ; done 
```

View the top of the new interleaved files to make sure your reads alternate between R1 and R2. 
```{bash, eval=FALSE}
grep @M interleaved.fastq | head
```


I recommend moving all the interleaved fastq to a new folder (I called the new folder `interleaved`)

<font color='red'>*I am looking for a better way to rename the interleave file from the loop. But this will do for now.*</font>

## Sickle 

Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3’-end of reads and also determines when the quality is sufficiently high enough to trim the 5’-end of reads. It generates two files for each fastq you input : `.trim.fastq` and `.singles.fastq`. We only need the `.trim.fastq` and therefor you can move all the others to another folder. 

Running sickle with a bash script. 

```{bash, eval=FALSE}
#!/bin/bash
for i in *.fastq
  do sickle pe -c $i -t sanger -m $i.trim.fastq -s $i.singles.fastq
done
```
*sickle pe (paired end) -c (inputfile) -t sanger (from illumina) -m (outputfilename) -s (exclutedreadsfilename)*


## Quality check with Fastqc 

Run fastqc on the output (trimmed) file and the non-trimmed file. The process can take a lot of time so a recommend using `nohup`. 

``````{bash, eval=FALSE}
#!/bin/bash 
fastqc *.fastq --outdir=/home/kvilleneuve/Metagenomic_analyses/interleaved/fastqc
``` 
Transfer the HTML files from the fastqc directory to your computer in order to view them in your a browser. 

## Fastq to Fasta 

Copy the interleave and trimmed fastq file to a new folder and then Gzip all and then convert them to fasta. I recommend using nohup and a bash script for both command since it can be very long. 
```{bash, eval=FALSE}
#!/bin/bash
gzip *.fastq
```

```{bash, eval=FALSE}
#!/bin/bash
for i in *.gz ; 
  do seqtk seq -a $i > $i.fa ; 
done 
```

***

# Assembly 

For the moment, there is no way of knowing which assembly program is best suited for your sample. Therefor, I try the different assemblers and look at (1) the output number of contigs, (2) the number of contigs > 2000 bp and (3) the number of contigs > 1000 bp. Because the assembly process is quite long I always use nohup. 

[Further reading](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-017-3918-9)

## IDBA 
[Github](https://github.com/loneknightpy/idba)
```{bash, eval=FALSE}
#!/bin/bash
for i in *.fa
  do idba_ud -l $i -o $i.assembly --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 i--num_threads 20
done
```

The output is a new folder ending in `assembly` for each of our file. In this folder we are interested in the file called `contig.fa`. 

## Megahit 
[Github](https://github.com/voutcn/megahit)
```{bash, eval=FALSE}
#!/bin/bash
for i in *.fa
  do megahit --12 $i --k-list 21,33,55,77,99,121 --min-count 2 --verbose -t 25 -o ~/Metagenomic_analyses/fasta/meg_$i --out-prefix megahit_$i  
done
``` 

## Metaspades 


## Spades 


***

# Post-Assembly stats 

## Number of contigs 
``````{bash, eval=FALSE}
grep -c ">" contig.fa 
```
## Lenght of contigs and histogram
Use seqkit to extract the length of every contigs. Remove the first row of the document lengths (word length) using vi and use pipeline to create histogram with the lengths file. 
``````{bash, eval=FALSE}
seqkit fx2tab --length --name --header-line contig.fa > length.tab 
# Open length.tab  in vi or nano and delete the first row 
cut -f 2 length.tab > lengths
```

<font color='red'>*Eventually remove this*</font> Replace with a script giving : **1**. The total amount of sequence **2**. Sequence > 2kbp **3**. Sequences > 1kbp **4**. Threshold for GC content ? 
``````{bash, eval=FALSE}
less lengths | Rscript -e 'data=abs(scan(file="stdin")); png("seq.png"); hist(data,xlab="sequences", breaks=250, xlim=c(0, 5000))'
```
The output is a png file called “seq.png”. If x axis of the histogram is not right change the xlim=c(x,x) values. Copy the file to your local computer to view it (in your local computer terminal navigate to the local directory where you want the file to be copied)

## Lenght and GC 
High GC organisms tend not to assemble well and may have an uneven read coverage distribution. I used this modified script `length+GC.pl`
``````{bash, eval=FALSE}
perl length+GC.pl contig.fa > contig_GC.txt
```

## Keeping sequences above 2000 base pairs (2kbp -kilobase pairs)  
``````{bash, eval=FALSE}
perl -lne 'if(/^(>.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (keys(%fa)){ print "$s\n$fa{$s}\n" if(length($fa{$s})>2000) }}' contig.fa > 2000bp.fa
``` 

# Binning 

Coverage-based binning approaches will require you to map reads to assembled contigs

## Mapping 

[Github](https://github.com/imrambo/genome_mapping)

This wrapper maps **FASTQ** reads against an assembly (e.g. genome) in **FASTA** format using BWA-MEM

For each sample, create a folder and copy into this folder these two files : the **long contig.fa** (1000 or 2000 bp) and the **trim and interleaved fastq** (fastq after sickle)

1. Go to your home directory and clone the git repository 

```{bash, highlight=TRUE, eval=FALSE}
git clone https://github.com/imrambo/genome_mapping.git
```

2. Go to the new directory created `genome_mapping° and create a conda environment 

```{bash, highlight=TRUE, eval=FALSE}
conda env create -f environment_linux64.yml
```

3. Activate the conda environement

```{bash, highlight=TRUE, eval=FALSE}
conda activate scons_map
```

3. Run a dry run to ensure everything runs smoothly  

```{bash, highlight=TRUE, eval=FALSE}
scons  --dry-run --fastq_dir=/home/karine/VP/megahit/concat --assembly=/home/karine/VP/megahit/concat/concatenated_1kb.fa --outdir=/home/karine/VP/megahit/concat/map --sampleids=fastq_concat.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/karine/tmp --logfile=concat.log
```

4. Run the script 

```{bash, highlight=TRUE, eval=FALSE}
scons  --fastq_dir=/home/karine/VP/SV10 --assembly=/home/karine/VP/SV10/SV10_1kb.fa --outdir=/home/karine/VP/SV10_map --sampleids=SV10_combined --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/karine/tmp --logfile=SV10log
```

In the outpir directory created  you will find a file that ends with `.sorted.bam` which we will need to create the depth file 

## Depth file {.tabset}

The depth allows you to know how many sequence you can align with certain sections of your contigs. Section with very little depth (few sequences) are not reputable to use

## MetaBAT 

[MetaBAT](https://peerj.com/articles/1165/)

Efficient tool for accurately reconstructing single genomes from complex microbial communities

a. Create `vi` file named `run_metabat.sh` with the following command

```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
metabat2 -i 2000kb.fa -a depth.txt -o bins_dir/bin -t 20 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000
```

`minCVsum` : assigning number of tetranucleotide frequency graphs, don’t grab negative numbers 
`-m` : min size of contig to be considered for binning

b. Run Metabat using `nohup ./`and `&`

The output is a folder called `bins_dir` containing all the bins created 

# Bin quality 

[Github](https://github.com/Ecogenomics/CheckM/wiki)

I downloaded the [checkm databases](https://data.ace.uq.edu.au/public/CheckM_databases/) and moved them to /usr/local/lib/checkm. I decompressed the file using `sudo tar -xf checkm_data_2015_01_16.tar.gz`. I changed the File Ownership to root `sudo chown -R root /usr/local/lib/checkm` and Group Ownership to me `sudo chgrp kvilleneuve /usr/local/lib/checkm`. I ran the following to inform CheckM of where the files have been placed: `checkm data setRoot /usr/local/lib/checkm`

You have to go back one folder in the terminal (not be in the bins_dir folder)

you  need to specify the extension of your file for it to work. For example, for file finishing is `.fa` the command will be `checkm lineage_wf -x fa`... 

```{bash, highlight=TRUE, eval=FALSE}
checkm lineage_wf -x fa bins_dir/ bins_dir/checkm -f bins_dir/output.txt
```

a. Open the `output.txt` document with excel to verify the **completeness** and **contamination** of your bins 

- Standard : Completeness > 50 % and Contamination < 10 % 

b. Remove all the spaces with `control` + `H`

c. Filter the columns by Completeness, and seperate the ones < 50 % by adding a line in excel 

d. Filter by Contamination, and highlight all the ones > 10 % - These are the bins you want to clean

# Bin cleaning 

# Taxonomy 

## GTDBTK

[Github](https://github.com/Ecogenomics/GTDBTk)

I located the folder with the untar GTDBTK data (GTDBTk_data/release95) and I added the path to this file to my ~/.profile (using vi)
```{bash, highlight=TRUE, eval=FALSE}
export GTDBTK_DATA_PATH=/home/kvilleneuve/GTDBTk_data/release95
```

In the folder with all your clean and completed genomes run this command with nohup

```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
gtdbtk classify_wf --cpus 20 --genome_dir /home/karine/Bins/all_clean --out_dir gtdbk_output -x fa
```

Once it is done running, you can open the folder called `gtdbk_output` and copy the folder `gtdbtk.bac120.summary.tsv` to your local computer in order to open it with excel. Use this folder to identify the phylum, class, order, family and genus that you need to download in order to construct your tree. 

# Phylogenetic tree 

# Metabolic pathway 

# Other information 

## Workflow (alternative)

[iMetAMOS](https://metamos.readthedocs.io/en/v1.5rc3/content/workflows.html)

## Articles 

New approaches for metagenome assembly with short reads. [Article](https://academic.oup.com/bib/article/21/2/584/5363831) by Ayling *et al.*, 2019. 

# Nohup 

Say you want to run the following command : `metabat2 -i 2000kb.fa -a depth.txt -o bins_dir/bin -t 20 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000` 

1. Create a vi document called `mapping.sh` with the following commmand : 
```{bash, eval=FALSE}
vi mapping.sh 
```
2. This creates a blank document in a vi text editor that is called `mapping.sh`. To start editing the document press `i`. Write the following command : 
```{bash, eval=FALSE}
#!/bin/bash 
metabat2 -i 2000kb.fa -a depth.txt -o bins_dir/bin -t 20 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000
```
3. To exit the vi document press the following keys `esc` , `:` , `w` , `q` 

4. Make the shell executable
```{bash, eval=FALSE}
chmod +x mapping.sh 
``` 
5. Run the shell script 
```{bash, eval=FALSE}
nohup ./mapping.sh & 
``` 
6. Press `enter` twice

If nohup exits there is likely a problem with the command you wrote in your shell script. To see the verbose and identify the source of the error open the `nohup.out` file (`less nohup.out`). To see if your task is still running use the command `jobs` 

