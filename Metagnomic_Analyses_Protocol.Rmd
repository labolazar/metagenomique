---
title: "Protocol for metagenomic analyses"
author: "Karine Villeneuve"
date: "`r Sys.Date()`"
output: rmdformats::robobook
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(eval = FALSE)
``` 

# Read me 

This protocol was created with the help of the baker lab team. Most of the programs are already installed on our server thanks to Mélanie Dominique and Karine Villeneuve. 

This protocol is intended to guide you through the different steps required to assemble and annotate reads from shotgun sequencing. 

Some program may take some time to process, we therefor recommend running the command using a shell script with nohup. For exemple on how to use, see section `Nohup`. 

# Pre-Assembly 

## Unzip 
Open a terminal window and move to the folder with your raw `fastq.gz`. 
```{bash, eval=FALSE}
gunzip *.gz
```

## Interleaving 
I used the script <font color='green'>interleave.py</font> from this [GitHub gist](https://gist.github.com/ngcrawford/2232505). The script is currently available in the folder `Script Métagénomique` of our Teams (see `General`). Transfer the script to the folder with the fastq files on the server, make the script executable and run the script using python. 

Make the script executable and chose option **a** or **b** depending on the amount of fastq you have. 
```{bash, eval=FALSE}
chmod +x interleave.py
```

a. If you have only a few fastq use this simple command line. 
```{bash, eval=FALSE}
python3 interleave.py file_R1.fastq file_R1_fastq > interleaved.fastq
```

b. If you have multiple fastq use this loop with nohup and a bash script.
``````{bash, eval=FALSE}
for R1 in *_R1_001.fastq ; do python3 interleave_fastq.py $R1 "${R1/R1/R2}"  > $R1.interleave.fastq ; done 
```

View the top of the new interleaved files to make sure your reads alternate between R1 and R2. 
```{bash, eval=FALSE}
grep @M interleaved.fastq | head
```

I recommend moving all the interleaved fastq to a new folder (I called the new folder `interleaved`)

<font color='red'>*I am looking for a better way to rename the interleave file from the loop. But this will do for now.*</font>

## Sickle 

Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3’-end of reads and also determines when the quality is sufficiently high enough to trim the 5’-end of reads. It generates two files for each fastq you input : `.trim.fastq` and `.singles.fastq`. We only need the `.trim.fastq` and therefor you can move all the others to another folder. 

Running sickle with a bash script. 

```{bash, eval=FALSE}
#!/bin/bash
for i in *.fastq
  do sickle pe -c $i -t sanger -m $i.trim.fastq -s $i.singles.fastq
done
```
*sickle pe (paired end) -c (inputfile) -t sanger (from illumina) -m (outputfilename) -s (exclutedreadsfilename)*

## Quality check with Fastqc 

Run fastqc on the output (trimmed) file and the non-trimmed file. The process can take a lot of time so a recommend using `nohup`. 

``````{bash, eval=FALSE}
#!/bin/bash 
fastqc *.fastq --outdir=/home/kvilleneuve/Metagenomic_analyses/interleaved/fastqc
``` 
Transfer the HTML files from the fastqc directory to your computer in order to view them in your a browser. 

## Fastq to Fasta 

Copy the interleave and trimmed fastq file to a new folder and then Gzip all and then convert them to fasta. I recommend using nohup and a bash script for both command since it can be very long. 
```{bash, eval=FALSE}
#!/bin/bash
gzip *.fastq
```

```{bash, eval=FALSE}
#!/bin/bash
for i in *.gz ; 
  do seqtk seq -a $i > $i.fa ; 
done 
```

***

# Assembly 

For the moment, there is no way of knowing which assembly program is best suited for your sample. Therefor, I try the different assemblers and look at (1) the output number of contigs, (2) the number of contigs > 2000 bp and (3) the number of contigs > 1000 bp. Because the assembly process is quite long I always use nohup. 

[About co-assembly](https://angus.readthedocs.io/en/2019/recovering-rep-genomes-from-mgs.html)

[Cross-Assembly pipeline](https://linsalrob.github.io/ComputationalGenomicsManual/CrossAssembly/)

[Further reading](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-017-3918-9)

## IDBA 
[Github](https://github.com/loneknightpy/idba)
```{bash, eval=FALSE}
#!/bin/bash
for i in *.fa
  do idba_ud -l $i -o $i.assembly --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 i--num_threads 20
done
```

The output is a new folder ending in `assembly` for each of our file. In this folder we are interested in the file called `contig.fa`. 

## Megahit 
[Github](https://github.com/voutcn/megahit)
```{bash, eval=FALSE}
#!/bin/bash
for i in *.fa
  do megahit --12 $i --k-list 21,33,55,77,99,121 --min-count 2 --verbose -t 25 -o ~/Metagenomic_analyses/fasta/meg_$i --out-prefix megahit_$i  
done
``` 

## Metaspades 
[Github](https://github.com/ablab/spades)
```{bash}
#!/bin/bash
for i in *.fastq 
  do metaspades.py --12 $i -o spades_$i 
done
``` 

First test with single file and only assembler 
```{bash}
metaspades.py --12 V01-2_S7_L001_R1_001.fastq.interleave.fastq.singles.fastq -s V01-2_S7_L001_R1_001.fastq.interleave.fastq.trim.fastq.gz.fa --only-assembler -t 50 -o V01-2_tests

Contigs : 660 405
````

Second test with forward and reverse reads
```{bash}
metaspades.py -1 V01-2_S7_L001_R1_001.fastq -2 V01-2_S7_L001_R2_001.fastq -t 50 -o output
``` 

# Submission to IMG/JGI 

The contig(s).fasta generated by the assembler (IDBA, Megahit, Metaspades or other) can be submitted to IMG/JGI for annotation of the contigs. For a step-by-step guide on how to submit see file `IMG_Submission` in Teams. 

### Co-assembly for indexing 

- Concat all the `interleave.trim.fastq` with `cat` and then run `metaspades.py` on this file. 
- Other option, concat all the forward reads and then all the reverse reads (to be tested). 

Use the ouput (contig.fa) as index for mapping reads (hoping to get better coverage for each contigs)

***

# Post-Assembly stats 

## Number of contigs 
``````{bash, eval=FALSE}
grep -c ">" contig.fa 
```
## Lenght of contigs and histogram
Use seqkit to extract the length of every contigs. Remove the first row of the document lengths (word length) using vi and use pipeline to create histogram with the lengths file. 
``````{bash, eval=FALSE}
seqkit fx2tab --length --name --header-line contig.fa > length.tab 
# Open length.tab  in vi or nano and delete the first row 
cut -f 2 length.tab > lengths
```

<font color='red'>*Eventually remove this*</font> Replace with a script giving : **1**. The total amount of sequence **2**. Sequence > 2kbp **3**. Sequences > 1kbp **4**. Threshold for GC content ? 
``````{bash, eval=FALSE}
less lengths | Rscript -e 'data=abs(scan(file="stdin")); png("seq.png"); hist(data,xlab="sequences", breaks=250, xlim=c(0, 5000))'
```
The output is a png file called “seq.png”. If x axis of the histogram is not right change the xlim=c(x,x) values. Copy the file to your local computer to view it (in your local computer terminal navigate to the local directory where you want the file to be copied)

## Lenght and GC 
High GC organisms tend not to assemble well and may have an uneven read coverage distribution. I used this modified script `length+GC.pl`
``````{bash, eval=FALSE}
perl /usr/local/bin/length+GC.pl contig.fa > contig_GC.txt
```

## Keeping sequences above 2000 base pairs (2kbp -kilobase pairs)  
``````{bash, eval=FALSE}
perl -lne 'if(/^(>.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (keys(%fa)){ print "$s\n$fa{$s}\n" if(length($fa{$s})>2000) }}' contig.fa > 2000bp.fa
``` 

perl -lne 'if(/^(>.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (keys(%fa)){ print "$s\n$fa{$s}\n" if(length($fa{$s})>1000) }}' coassembled_contigs.fa > coassemble_1kb.fa

# Binning 

In metagenomics, binning is the process of grouping reads or contigs and assigning them to individual genome known as Metagenome Assembled Genome (MAG). Coverage-based binning approaches will require you to map reads to assembled contigs. 

[Comparision of different binning tools](https://bitbucket.org/berkeleylab/metabat/wiki/Home)

## Mapping 

Mapping allows you to get a rough count of the total number of reads mapping to each contig, also referred to as the depth file or read coverage. This information is required for most binning algorithm. Raw reads from each sample are mapped to the contigs (assembled reads).The idea being that if two contigs come from the same genome in your sample, so the same organism, then they would have been sequenced roughly to the same depth, they would have a similar coverage and they go together. It gets even better if you have multiple samples that vary a bit and you sequence all of them. You do your assembly on one of the samples, then you can take the reads from the other samples, map to that genome (metagenomic assembly) and you can use that as additional information. 

Different tools exists for mapping reads to genomic sequences. Here you have three ways of mapping you reads : (**1**) mostly pre-scripted using a conda virtual environment (Scons wrapper) (**2**) mostly manual commands that may require more time using BWA and Samtools, (**3**) BBMap.

<font color='red'> My question here is what do we index? Do we index every assembly from the different samples individually or do co-assembled the assembly and index it?</font> **I believe it is mapped on co-assembly. 

### Scons Wrapper 

[Github](https://github.com/imrambo/genome_mapping)

This wrapper maps **FASTQ** reads against an assembly (e.g. genome) in **FASTA** format using BWA-MEM. This wrapper does not produce huge intermediate files (e.g. unfiltered SAM). 

For each sample, create a folder and copy into this folder these two files : the **long contig.fa** (1000 or 2000 bp) and the **trim and interleaved fastq** (fastq after sickle)

1. Go to your home directory and clone the git repository 

```{bash, highlight=TRUE, eval=FALSE}
git clone https://github.com/imrambo/genome_mapping.git
```

2. Go to the new directory created `genome_mapping` and create a conda environment (**if you have already created this environnement in the past skip to next step**)

```{bash, highlight=TRUE, eval=FALSE}
conda env create -f environment_linux64.yml
```

3. Activate the conda environement

```{bash, highlight=TRUE, eval=FALSE}
conda activate scons_map
```

3. Run a dry run to ensure everything runs smoothly (you must run this from the `genome_mapping` directory)

```{bash, highlight=TRUE, eval=FALSE}
scons --dry-run --fastq_dir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/ --assembly=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/V01_2_1000bp.fa --outdir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/output --sampleids=V01_2_S7_L001_R1_001.fastq.interleave.fastq.trim.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/kvilleneuve/tmp --logfile=mapping.log
```

4. Run the script (you must run this from the `genome_mapping` directory)

```{bash, highlight=TRUE, eval=FALSE}
scons --fastq_dir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/ --assembly=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/V01_2_1000bp.fa --outdir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/output --sampleids=V01_2_S7_L001_R1_001.fastq.interleave.fastq.trim.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/kvilleneuve/tmp --logfile=mapping.log
```

<font color='red'>Currently looking for a way to loop this. Tried specifying only the imput folder and the sampleids as proposed in the README but it does not work. `sampleids` **must** be the exact name as the `fastq` or `fastq.gz` file.</font>

```{bash, highlight=TRUE, eval=FALSE}
scons --fastq_dir=/home/kvilleneuve/Metagenomic_analyses/mapping/ --assembly=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2_1000bp.fa --outdir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01-2_output --sampleids=V01-2.fastq.gz --align_thread=20 --samsort_thread=20 --samsort_mem=768M --nheader=8 --tmpdir=/home/kvilleneuve/tmp --logfile=mapping.log
```

In the output directory created  you will find a file that ends with `.sorted.bam` which we will need to create the depth file 

### Manually with BWA and SamTOOLS 

Not sure how I should proceed, but I concat all the .fa and indexed that file (concat_1kb.fa)

1. Index your assembly (newly assembled contig.fa, either 1000 bp or 2000 bp). 

```{bash, highlight=TRUE, eval=FALSE}
bwa index contig.fa
```

for multiple files use vi and nohup. 
```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
for i in *.fa 
  do bwa index -p index_$i $i 
done
```

2. Align the fasta file against the interleaved and trimmed fastq file. Before hand I renamed all my files in this format : `nameofthefafile.fa.interleaved.trim.fastq` in order to be able to loop the BWA command.  

- Create a `vi`file named `mapping.sh`with the following script 

```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
bwa mem -t 30 2000kb.fa -p MF_combined_trimmed_fastq > 2KB.MF.sam
```

  - Make the mapping.sh script executable `chmod +x mapping.sh`
  - Run the script in the background using `nohup ./` and `&` 
 
```{bash, highlight=TRUE, eval=FALSE}
nohup ./mapping.sh & 
```   

**c. Convert `.sam` file to `.bam` file with** [samtools](http://www.metagenomics.wiki/tools/samtools)

- Create a vi file named `samtools.sh` with the following script 
```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
samtools view -b -S 2KB.MF.sam > 2KB.MF.bam
``` 

- `-S`input : `.sam` file 
- `-b`output : `.bam`file

**d. Sort the bam file** 

Required in order to use the script to generate a depth file 

- create a vi file named `sort.sh` with the following script 
```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash 
samtools sort -o 2KB.MF.sorted.bam 2KB.MF.bam
``` 
 - Make the script executable `chmod +x sort.sh`
 - Run the script in the background using `nohup ./` and `&`

### BBMap 

[BBMap](https://jgi.doe.gov/data-and-tools/bbtools/): Short read aligner for DNA and RNA-seq data. Capable of handling arbitrarily large genomes with millions of scaffolds. Handles Illumina, PacBio, 454, and other reads; very high sensitivity and tolerant of errors and numerous large indels. Very fast. 

- Install : `conda install -c bioconda bbmap` do not install with `sudo apt get bbmap.sh`
- Requires the conda environment to be activated (`conda activate`)

*Create a folder with all your assembled contigs and your interleave.trim.fastq*

**Step-by-step for single sample** 

1. Index the contigs 
```{bash}
bbmap.sh ref=contigs.fa
```

The index is written to the location `/ref/`. In other words, if you run BBMap from the location /bob/work/, then the directory /bob/work/ref/ will be created and an index written to it. 

2. Map the reads 
```{bash}
bbmap.sh in=...interleave.trim.fastq out=megahit.sam bamscript=sam2bam.sh
```

This produces output in SAM format by default, usually you want to convert this into a sorted BAM file. bbmap creates a shell script which can be used to convert bbmap‘s output into BAM format:

3. Convert to bam 
```{bash}
source sam2bam.sh
``` 

**Shell script for multiple samples** 

```{bash}
#!/bin/bash
for i in *.fa 
  do bbmap.sh ref=$i in=$i.interleave.trim.fastq out=$i.sam bamscript=$i.sam2bam.sh 
done
``` 

## Depth file

The depth allows you to know how many sequence you can align with certain sections of your contigs. Section with very little depth (few sequences) are not reputable to use. Requires the use of the `jgi_summarize_bam_contig_depths` which is available in Teams. 

```{bash}
#!/bin/bash
for i in *.bam
  do ./jgi_summarize_bam_contig_depths --outputDepth $i.depth.txt --pairedContigs $i.paired.txt $i
done 
```

## MetaBAT2 

[Github](https://bitbucket.org/berkeleylab/metabat/src/master/) / [Artcile](https://peerj.com/articles/1165/)
Efficient tool for accurately reconstructing single genomes from complex microbial communities

MetaBAT2 requires that your python environment be activate (base). First deactivate scons_map and then activate conda
```{bash}
conda deactivate
conda activate
```

In order to be able to loop this for all my samples, I renamed each depth file in the following format : `name_of_1000_b_pfile.fa.depth.txt`

a. Create `vi` file named `run_metabat.sh` with the following command

```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
metabat2 -i 2000kb.fa -a depth.txt -o bins_dir/bin -t 0 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 1000
```

b. Run Metabat using `nohup ./`and `&`

The output is a folder called `bins_dir` containing all the bins created 

Bash for multiple samples 
```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
for i in *.fa
  do metabat2 -i $i -a $i.depth.txt -o /home/kvilleneuve/Metagenomic_analyses/binning/$i_bins -t 0 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000
done 
```

`minCVsum` : assigning number of tetranucleotide frequency graphs, don’t grab negative numbers 
`-m` : min size of contig to be considered for binning

## CONCOT 

## Testing different assembler 
[anvio-7](https://merenlab.org/2016/06/22/anvio-tutorial-v2/)

# Bin quality 

[Github](https://github.com/Ecogenomics/CheckM/wiki)

If checkm doesn't work for you : 
```{bash}
pip3 install checkm-genome --upgrade --no-deps
pip3 install pysam
pip3 install checkm-genome
checkm data setRoot /usr/local/lib/checkm
```

I downloaded the [checkm databases](https://data.ace.uq.edu.au/public/CheckM_databases/) and moved them to /usr/local/lib/checkm. I decompressed the file using `sudo tar -xf checkm_data_2015_01_16.tar.gz`. I changed the File Ownership to root `sudo chown -R root /usr/local/lib/checkm` and Group Ownership to me `sudo chgrp kvilleneuve /usr/local/lib/checkm`. I ran the following to inform CheckM of where the files have been placed: `checkm data setRoot /usr/local/lib/checkm`

You have to go back one folder in the terminal as checkm will run on all the files in the folder you give it as input. If you must run checkm again make sure the delete the newly create checkm folder, otherwise checkm will give you an error message. 

You  need to specify the extension of your file for it to work. For example, for file finishing is `.fa` the command will be `checkm lineage_wf -x fa`... 

If you have many bins I recommend using nohup as it is a timely process. 

```{bash, highlight=TRUE, eval=FALSE}
checkm lineage_wf -x fa bins_dir/ bins_dir/checkm -f bins_dir/output.txt
```

a. Open the `output.txt` document with excel to verify the **completeness** and **contamination** of your bins 

- Standard : Completeness > 50 % and Contamination < 10 % 

b. Remove all the spaces with `control` + `H`

c. Filter the columns by Completeness, and seperate the ones < 50 % by adding a line in excel 

d. Filter by Contamination, and highlight all the ones > 10 % - These are the bins you want to clean

## Bin cleaning 

### Vizbin 

### MMGenome 

The [mmgenome](http://madsalbertsen.github.io/mmgenome/) toolbox enables reproducible extraction of individual genomes from metagenomes. It builds on the multi-metagenome concept, but wraps most of the process of extracting genomes in simple R functions. Thereby making the whole process of binning easy and at the same time reproducible through the Rmarkdown format. [Article](https://www.biorxiv.org/content/10.1101/059121v1)


# Taxonomy 

## GTDBTK

[Github](https://github.com/Ecogenomics/GTDBTk)

I located the folder with the untar GTDBTK data (GTDBTk_data/release95) and I added the path to this file to my ~/.profile (using vi)
```{bash, highlight=TRUE, eval=FALSE}
export GTDBTK_DATA_PATH=/home/kvilleneuve/GTDBTk_data/release95
```

In the folder with all your clean and completed genomes run this command with nohup

```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
gtdbtk classify_wf --cpus 20 --genome_dir /home/karine/Bins/all_clean --out_dir gtdbk_output -x fa
```

Once it is done running, you can open the folder called `gtdbk_output` and copy the folder `gtdbtk.bac120.summary.tsv` to your local computer in order to open it with excel. Use this folder to identify the phylum, class, order, family and genus that you need to download in order to construct your tree. 

# Phylogenetic tree 

# Metabolic pathway 

# Other information 

## Workflow (alternative)

[iMetAMOS](https://metamos.readthedocs.io/en/v1.5rc3/content/workflows.html)

## Articles 

New approaches for metagenome assembly with short reads. [Article](https://academic.oup.com/bib/article/21/2/584/5363831) by Ayling *et al.*, 2019. 

# Nohup 

Say you want to run the following command : `metabat2 -i 2000kb.fa -a depth.txt -o bins_dir/bin -t 20 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000` 

1. Create a vi document called `mapping.sh` with the following commmand : 
```{bash, eval=FALSE}
vi mapping.sh 
```
2. This creates a blank document in a vi text editor that is called `mapping.sh`. To start editing the document press `i`. Write the following command : 
```{bash, eval=FALSE}
#!/bin/bash 
metabat2 -i 2000kb.fa -a depth.txt -o bins_dir/bin -t 20 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000
```
3. To exit the vi document press the following keys `esc` , `:` , `w` , `q` 

4. Make the shell executable
```{bash, eval=FALSE}
chmod +x mapping.sh 
``` 
5. Run the shell script 
```{bash, eval=FALSE}
nohup ./mapping.sh & 
``` 
6. Press `enter` twice

If nohup exits there is likely a problem with the command you wrote in your shell script. To see the verbose and identify the source of the error open the `nohup.out` file (`less nohup.out`). To see if your task is still running use the command `jobs` 

# Further reading and online tutorials 

https://datacarpentry.org/wrangling-genomics/aio.html 

https://bioinformaticsworkbook.org/dataAnalysis/Metagenomics/MetagenomicsP1.html#gsc.tab=0 
